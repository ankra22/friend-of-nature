import chromadb
import os
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_groq import ChatGroq
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv

load_dotenv()

DB_FOLDER = r"C:\chroma\banco"

COLLECTION_NAME = "PlanoManejo_Tijuca"
TOP_K = 5  

groq_api_key = os.getenv("GROQ_API_KEY")
if not groq_api_key:
    print("\n‚ö†Ô∏è  AVISO: Verifique se a GROQ_API_KEY est√° configurada corretamente")
    exit(1)

# inicializa llm do groq
llm = ChatGroq(
    groq_api_key=groq_api_key,
    model_name="llama-3.3-70b-versatile",
    temperature=0.3,
    max_tokens=2000
)

# inicializa os embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)


def inicializar_vectorstore():
    # usando langchain concecta o banco de dados
    if not os.path.exists(DB_FOLDER):
        raise FileNotFoundError(
            f"Banco de dados n√£o encontrado em: {DB_FOLDER}\n"
            "Execute primeiro o script de processamento dos PDFs."
        )

    try:
        # concecta o chromadb
        vectorstore = Chroma(
            collection_name=COLLECTION_NAME,
            embedding_function=embeddings,
            persist_directory=DB_FOLDER
        )

        # verifica a quantidade de documentos que tem na cole√ß√£o
        collection = vectorstore._collection
        _ = collection.count()  # s√≥ para validar que a cole√ß√£o responde

        return vectorstore

    except Exception as e:
        raise Exception(f"Erro ao acessar cole√ß√£o: {e}")


def criar_prompt_template():
    # template do agente com hist√≥rico

    template = """Voc√™ √© um guia experiente do Parque Nacional da Tijuca, no Rio de Janeiro.

Use uma linguagem clara, organizada e acolhedora, mas sem exageros e sem v√≠cios de linguagem.
Evite express√µes como "olha s√≥", "sabe o que √© incr√≠vel?", "cara, isso √© demais", "vem comigo que eu te conto"
ou qualquer outra muleta de linguagem repetitiva.

ESTILO DA RESPOSTA:
- Explique as informa√ß√µes de forma direta, mas simp√°tica.
- Use par√°grafos curtos, bem organizados, evitando repetir a mesma ideia v√°rias vezes.
- Procure responder em at√© 2 ou 3 par√°grafos, com no m√°ximo 8 frases no total.
- Se usar emojis, use no m√°ximo 2 por resposta, apenas ao final de frases ou par√°grafos, nunca no meio da frase.
- Nunca comece a resposta com emoji; primeiro o texto, depois o emoji, se fizer sentido.

VOC√ä PODE FALAR SOBRE:
- Fauna (animais, comportamentos, onde √© mais comum avistar)
- Flora (esp√©cies nativas, √°rvores marcantes, import√¢ncia ecol√≥gica)
- Trilhas e pontos tur√≠sticos (dificuldade, tempo m√©dio, cuidados)
- Hist√≥ria do parque (reflorestamento, contexto hist√≥rico)
- Dicas pr√°ticas (hor√°rios, seguran√ßa, o que levar, regras gerais)

REGRAS IMPORTANTES:
1. Baseie TUDO no contexto dos documentos fornecidos ‚Äì voc√™ √© um guia respons√°vel.
2. Se n√£o souber algo, responda de forma honesta: por exemplo,
   "N√£o encontrei essa informa√ß√£o nos documentos do parque, mas posso te explicar sobre outro aspecto relacionado."
3. Considere o hist√≥rico da conversa ‚Äì voc√™ lembra do que j√° conversaram.
4. Quando fizer refer√™ncia √†s fontes, fale de forma natural, como
   "De acordo com o plano de manejo do parque..." ou "Nos documentos oficiais do parque √© citado que...".
5. Explique termos t√©cnicos de forma simples, evitando jarg√µes sem explica√ß√£o.
6. NUNCA invente informa√ß√µes ‚Äì preserve a credibilidade do guia.

CONTEXTO DOS DOCUMENTOS:
{context}

PERGUNTA DO VISITANTE: {question}"""

    return ChatPromptTemplate.from_messages([
        ("system", template),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}")
    ])


def criar_chain_rag(vectorstore):
    # cria a chain RAG

    from langchain_core.output_parsers import StrOutputParser

    # retriver
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": TOP_K}
    )

    # prompt
    prompt = criar_prompt_template()

    # formata√ß√£o de documentos
    def format_docs(docs):
        return "\n\n".join([
            f"[Fonte: {doc.metadata.get('arquivo', 'Desconhecido')} - Parte {doc.metadata.get('parte', '?')}]\n{doc.page_content}"
            for doc in docs
        ])

    # chain(LCEL) com hist√≥rico
    retrieval_chain = (
        {
            "context": lambda x: format_docs(retriever.invoke(x["question"])),
            "question": lambda x: x["question"],
            "chat_history": lambda x: x.get("chat_history", [])
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return retrieval_chain, retriever


def processar_pergunta_langchain(chain_tuple, pergunta, chat_history=None):
    # usa a chain pra processar a pergunta

    chain, retriever = chain_tuple

    if chat_history is None:
        chat_history = []

    print(f"Pergunta: {pergunta}\n")

    try:
        # busca documentos relevantes
        documentos = retriever.invoke(pergunta)

        # executa a chain para gerar a resposta com hist√≥rico
        resposta = chain.invoke({
            "question": pergunta,
            "chat_history": chat_history
        })

        # resposta no terminal
        print("Resposta:\n")
        print(resposta)
        print()

        # atualiza√ß√£o do hist√≥rico
        chat_history.append(HumanMessage(content=pergunta))
        chat_history.append(AIMessage(content=resposta))

        return resposta, documentos, chat_history

    except Exception as e:
        print(f"‚ùå Erro ao processar pergunta: {e}")
        import traceback
        traceback.print_exc()
        return None, [], chat_history


def modo_interativo():
    # modo interativo para fazer diferentes perguntas
    print("=" * 70)
    print("üåø Guia virtual do Parque Nacional da Tijuca")
    print("=" * 70)
    print("Posso te ajudar com:\n")
    print("  ‚Ä¢ Informa√ß√µes detalhadas sobre fauna e flora")
    print("  ‚Ä¢ Informa√ß√µes detalhadas sobre regras do parque")

    try:
        vectorstore = inicializar_vectorstore()
        chain_tuple = criar_chain_rag(vectorstore)
    except Exception as e:
        print(f"‚ùå {e}")
        import traceback
        traceback.print_exc()
        return

    chat_history = []

    print("\n" + "=" * 70)
    print(" COMANDOS DISPON√çVEIS:")
    print("  ‚Ä¢ Digite sua pergunta normalmente")
    print("  ‚Ä¢ 'sair' - encerrar o programa")
    print("  ‚Ä¢ 'limpar' - resetar hist√≥rico da conversa")
    print("=" * 70 + "\n")

    while True:
        try:
            pergunta = input("üåø Sua pergunta: ").strip()

            if not pergunta:
                continue

            if pergunta.lower() in ["sair", "Sair"]:
                print("\n" + "=" * 70)
                print("üëã Obrigado por usar o Guia do Parque Nacional da Tijuca!")
                print("   Aproveite sua aventura na natureza! üåøüèûÔ∏è")
                print("=" * 70 + "\n")
                break

            if pergunta.lower() in ["limpar", "Limpar"]:
                chat_history = []
                print("\nüóëÔ∏è Hist√≥rico de conversa limpo!\n")
                continue

            resposta, docs, chat_history = processar_pergunta_langchain(
                chain_tuple,
                pergunta,
                chat_history
            )

        except KeyboardInterrupt:
            print("\n\nüëã At√© logo!\n")
            break
        except Exception as e:
            print(f"\n‚ùå Erro: {e}\n")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    modo_interativo()
